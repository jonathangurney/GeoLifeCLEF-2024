{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efc2006-cad4-4392-a25b-c77cc18b5da8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a199a7e-94b9-4ac4-8ed4-2dec46d3fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './kaggle/input/geolifeclef-2024'\n",
    "output_path = './kaggle/working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0915bb1c-3364-4c7e-bb33-4778cc7f59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PA metadata for the training set\n",
    "pa_metadata_train_csv_filename = data_path + '/GLC24_PA_metadata_train.csv'\n",
    "pa_metadata_train_df = pd.read_csv(pa_metadata_train_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3533dfd1-a34a-4e1c-bc0a-abe466405a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about each survey for plotting locations, including the number\n",
    "# of each species found in each survey\n",
    "pa_survey_metadata_train_df = (pa_metadata_train_df\n",
    "                               .copy()\n",
    "                               .drop_duplicates('surveyId'))\n",
    "pa_survey_metadata_train_df.drop('speciesId', axis=1, inplace=True)\n",
    "pa_survey_metadata_train_df['speciesCount'] = (pa_metadata_train_df\n",
    "                                        .groupby('surveyId')\n",
    "                                        .count()\n",
    "                                        .speciesId\n",
    "                                        .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9f72ea-d687-4e03-9cc7-2801501eed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environmental raster file paths\n",
    "climate_raster_path = data_path + '/EnvironmentalRasters/EnvironmentalRasters/Climate'\n",
    "elevation_raster_path = data_path + '/EnvironmentalRasters/EnvironmentalRasters/Elevation'\n",
    "human_raster_path = data_path + '/EnvironmentalRasters/EnvironmentalRasters/Human Footprint'\n",
    "land_cover_raster_path = data_path + '/EnvironmentalRasters/EnvironmentalRasters/LandCover'\n",
    "soil_grid_raster_path = data_path + '/EnvironmentalRasters/EnvironmentalRasters/SoilGrids'\n",
    "\n",
    "# Load the climate raster csv file\n",
    "average_climate_raster_train_csv_filename = (climate_raster_path + \n",
    "                                             '/Average 1981-2010/GLC24-PA-train-bioclimatic.csv')\n",
    "monthly_climate_raster_train_csv_filename = (climate_raster_path + \n",
    "                                             '/Monthly/GLC24-PA-train-bioclimatic_monthly.csv')\n",
    "\n",
    "average_climate_raster_train_df = pd.read_csv(average_climate_raster_train_csv_filename)\n",
    "monthly_climate_raster_train_df = pd.read_csv(monthly_climate_raster_train_csv_filename)\n",
    "\n",
    "pa_elevation_train_csv_filename = (elevation_raster_path + \n",
    "                                   '/GLC24-PA-train-elevation.csv')\n",
    "pa_elevation_train_df = pd.read_csv(pa_elevation_train_csv_filename)\n",
    "\n",
    "pa_elevation_train_df['has_missing'] = pa_elevation_train_df.isnull().any(axis=1)\n",
    "\n",
    "pa_human_train_csv_filename = (human_raster_path + \n",
    "                               '/GLC24-PA-train-human_footprint.csv')\n",
    "pa_human_train_df = pd.read_csv(pa_human_train_csv_filename)\n",
    "\n",
    "pa_human_train_df['has_missing'] = pa_human_train_df.isnull().any(axis=1)\n",
    "\n",
    "pa_land_cover_train_csv_filename = (land_cover_raster_path + \n",
    "                                   '/GLC24-PA-train-landcover.csv')\n",
    "pa_land_cover_train_df = pd.read_csv(pa_land_cover_train_csv_filename)\n",
    "\n",
    "pa_soil_grid_train_csv_filename = (soil_grid_raster_path + \n",
    "                                   '/GLC24-PA-train-soilgrids.csv')\n",
    "pa_soil_grid_train_df = pd.read_csv(pa_soil_grid_train_csv_filename)\n",
    "\n",
    "pa_soil_grid_train_df['has_missing'] = pa_soil_grid_train_df.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a41cbd-6895-4b53-8dd1-d8468956af1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T21:34:19.825297Z",
     "iopub.status.busy": "2024-04-24T21:34:19.824626Z",
     "iopub.status.idle": "2024-04-24T21:34:19.836091Z",
     "shell.execute_reply": "2024-04-24T21:34:19.835213Z",
     "shell.execute_reply.started": "2024-04-24T21:34:19.825264Z"
    }
   },
   "source": [
    "## Landsat Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe581d3-3125-42f3-aa81-4300bcca597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_train_path = data_path + '/PA-train-landsat_time_series'\n",
    "\n",
    "blue_landsat_train_csv_filename = landsat_train_path + '/GLC24-PA-train-landsat_time_series-blue.csv'\n",
    "green_landsat_train_csv_filename = landsat_train_path + '/GLC24-PA-train-landsat_time_series-green.csv'\n",
    "red_landsat_train_csv_filename = landsat_train_path + '/GLC24-PA-train-landsat_time_series-red.csv'\n",
    "nir_landsat_train_csv_filename = landsat_train_path + '/GLC24-PA-train-landsat_time_series-nir.csv'\n",
    "swir1_landsat_train_csv_filename = landsat_train_path + '/GLC24-PA-train-landsat_time_series-swir1.csv'\n",
    "swir2_landsat_train_csv_filename = landsat_train_path + '/GLC24-PA-train-landsat_time_series-swir2.csv'\n",
    "\n",
    "blue_landsat_train_df = pd.read_csv(blue_landsat_train_csv_filename)\n",
    "green_landsat_train_df = pd.read_csv(green_landsat_train_csv_filename)\n",
    "red_landsat_train_df = pd.read_csv(red_landsat_train_csv_filename)\n",
    "nir_landsat_train_df = pd.read_csv(nir_landsat_train_csv_filename)\n",
    "swir1_landsat_train_df = pd.read_csv(swir1_landsat_train_csv_filename)\n",
    "swir2_landsat_train_df = pd.read_csv(swir2_landsat_train_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3280dfb0-7a62-42f7-8efe-acf4cebe9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the species/survey matrix\n",
    "max_species_id = pa_metadata_train_df.speciesId.max()\n",
    "pa_species_df = pd.DataFrame(0, \n",
    "                             index=pa_metadata_train_df.surveyId.unique(),\n",
    "                             columns=list(range(1, int(max_species_id+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e864ec09-f88b-4a0b-9fd1-27636e2434f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate (speciesId, surveyId) entries\n",
    "pa_metadata = pa_metadata_train_df.drop_duplicates(['speciesId', 'surveyId']).copy()\n",
    "\n",
    "# Change the speciesId dtype to int\n",
    "pa_metadata['speciesId'] = pa_metadata['speciesId'].apply(int)\n",
    "\n",
    "surveyId_map = {idx: i for i, idx in enumerate(pa_metadata.surveyId.unique())}\n",
    "\n",
    "# Set the values in the species/survey matrix\n",
    "pa_species_df.values[pa_metadata.surveyId.map(surveyId_map), \n",
    "                     pa_metadata.speciesId.apply(lambda x: x-1).tolist()] = 1\n",
    "\n",
    "pa_species_df.reset_index(names='surveyId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a53e19e-248b-44da-8282-f978eff4eadb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for 'country' and 'region' variables\n",
    "pa_survey_metadata_train_df = pd.merge(pa_survey_metadata_train_df,\n",
    "                                       pd.get_dummies(pa_survey_metadata_train_df[['country', 'region']], dtype='int'),\n",
    "                                       how='left',\n",
    "                                       left_index=True,\n",
    "                                       right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ac0061-ac95-4541-b4bf-c737350c27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes into a single, larger dataframe.\n",
    "\n",
    "# To avoid any potential issues with data leakage I have cut off the monthly climate raster dataframe\n",
    "# at the end of 2016.\n",
    "\n",
    "monthly_climate_end_idx = 817\n",
    "\n",
    "pa_train_df = pd.merge(pa_survey_metadata_train_df.drop(['areaInM2', 'geoUncertaintyInM'], axis=1),\n",
    "                      monthly_climate_raster_train_df.iloc[:, 0:monthly_climate_end_idx],\n",
    "                      how='left',\n",
    "                      on='surveyId')\n",
    "\n",
    "pa_train_df = pd.merge(pa_train_df,\n",
    "                      average_climate_raster_train_df,\n",
    "                      how='left',\n",
    "                      on='surveyId')\n",
    "\n",
    "pa_train_df = pd.merge(pa_train_df,\n",
    "                       pa_elevation_train_df.drop('has_missing', axis=1),\n",
    "                       how='left',\n",
    "                       on='surveyId')\n",
    "\n",
    "pa_train_df = pd.merge(pa_train_df,\n",
    "                       pa_human_train_df.drop('has_missing', axis=1),\n",
    "                       how='left',\n",
    "                       on='surveyId')\n",
    "\n",
    "pa_train_df = pd.merge(pa_train_df,\n",
    "                       pa_land_cover_train_df,\n",
    "                       how='left',\n",
    "                       on='surveyId')\n",
    "\n",
    "pa_train_df = pd.merge(pa_train_df,\n",
    "                       pa_soil_grid_train_df.drop('has_missing', axis=1),\n",
    "                       how='left',\n",
    "                       on='surveyId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931a4acf-eeda-4934-8220-98abb5b8ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_landsat_data(row, band, year_df):\n",
    "    year = year_df.loc[year_df.surveyId == row.surveyId, \"year\"].values[0]\n",
    "    start_idx = (year - 2017)*4 + 1\n",
    "    end_idx = start_idx + 68\n",
    "    obs = row[start_idx:end_idx]\n",
    "    obs.index = [f\"{band}_{lag}\" for lag in list(range(68, 0, -1))]\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2519d685-8b57-4773-8c88-022108b89543",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_df_list = [blue_landsat_train_df,\n",
    "                   green_landsat_train_df,\n",
    "                   red_landsat_train_df,\n",
    "                   nir_landsat_train_df,\n",
    "                   swir1_landsat_train_df,\n",
    "                   swir2_landsat_train_df]\n",
    "\n",
    "landsat_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "\n",
    "for band, landsat_df in zip(landsat_bands, landsat_df_list):\n",
    "    landsat_merge_df = landsat_df.apply(reindex_landsat_data,\n",
    "                                        axis=1,\n",
    "                                        band=band,\n",
    "                                        year_df=pa_survey_metadata_train_df)\n",
    "    pa_train_df = pd.merge(pa_train_df,\n",
    "                           landsat_merge_df,\n",
    "                           how='left',\n",
    "                           left_index=True,\n",
    "                           right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f458f56c-8de7-4ed8-927c-48c2ffd99b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial interpolation to fill missing values. I will do this by creating a grid and using\n",
    "# the mean values from the grid boxes to fill any null values\n",
    "lat_n_gridlines = 201\n",
    "lon_n_gridlines = 201\n",
    "lat_min = pa_survey_metadata_train_df.lat.min()\n",
    "lat_max = pa_survey_metadata_train_df.lat.max()\n",
    "lon_min = pa_survey_metadata_train_df.lon.min()\n",
    "lon_max = pa_survey_metadata_train_df.lon.max()\n",
    "\n",
    "# lat_gridlines = np.linspace(lat_min, lat_max, lat_n_gridlines)\n",
    "# lon_gridlines = np.linspace(lon_min, lon_max, lon_n_gridlines)\n",
    "lat_grid = pd.interval_range(lat_min, lat_max, periods=(lat_n_gridlines-1), closed='both')\n",
    "lon_grid = pd.interval_range(lon_min, lon_max, periods=(lon_n_gridlines-1), closed='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a72a96b-70a4-486b-9832-4f9ddf0d9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_to_grid_num(row):\n",
    "    lat_num = np.array(range(lat_n_gridlines-1))[lat_grid.contains(row.lat)][0]\n",
    "    lon_num = np.array(range(lon_n_gridlines-1))[lon_grid.contains(row.lon)][0]\n",
    "        \n",
    "    grid_num = lat_num*(lon_n_gridlines - 1) + lon_num\n",
    "    return grid_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47be0045-d065-40c8-96c6-b6786011a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_train_df['grid_num'] = pa_train_df.apply(loc_to_grid_num, axis=1)\n",
    "\n",
    "grid_num_counts = pa_train_df['grid_num'].value_counts()\n",
    "single_grid_locs = grid_num_counts.index[grid_num_counts == 1]\n",
    "\n",
    "single_null_locations = pa_train_df.index[(pa_train_df.isnull().any(axis=1) & \n",
    "                                           pa_train_df.grid_num.isin(single_grid_locs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d72ef55a-b281-4efc-9feb-fe6493ec3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the grid numbers which border the current one\n",
    "def bordering_grid_nums(grid_num, lat_n_gridlines, lon_n_gridlines):\n",
    "    n_rows = lat_n_gridlines - 1\n",
    "    n_cols = lon_n_gridlines - 1\n",
    "    \n",
    "    if grid_num < 0:\n",
    "        raise ValueError(\"grid number should be non-negative\")\n",
    "    if grid_num > n_rows*n_cols - 1:\n",
    "        raise ValueError(\"grid number is too large\")\n",
    "        \n",
    "    col = grid_num % (n_cols)\n",
    "    row = grid_num // (n_cols)\n",
    "    \n",
    "    left_col, right_col = 0, 3\n",
    "    top_row, bottom_row = 0, 3\n",
    "    \n",
    "    if (col % (n_cols)) == 0:\n",
    "        left_col = 1\n",
    "    if (col % (n_cols)) == lon_n_gridlines - 2:\n",
    "        right_col = 2\n",
    "    if (row % (n_rows)) == 0:\n",
    "        top_row = 1\n",
    "    if (row % (n_rows)) == lat_n_gridlines - 2:\n",
    "        bottom_row = 2\n",
    "        \n",
    "    return [grid_num + n1*n_rows + n2 for n1 in [-1, 0, 1][top_row:bottom_row] for n2 in [-1, 0, 1][left_col:right_col] if (n1 != 0 or n2 != 0)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a565e2fa-602d-4a87-897c-ad7034967ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12390/12390 [01:22<00:00, 150.88it/s]\n"
     ]
    }
   ],
   "source": [
    "null_idxs = pa_train_df.index[pa_train_df.isnull().any(axis=1)]\n",
    "for idx in tqdm(null_idxs):\n",
    "    row = pa_train_df.loc[idx]\n",
    "    missing_cols = row.index[row.isnull()]\n",
    "    \n",
    "    # If grid cell means contain null values take values from the bordering cells\n",
    "    groupby_mean_df = pa_train_df.groupby('grid_num')[missing_cols].mean()\n",
    "    if groupby_mean_df.loc[row.grid_num].isnull().any():\n",
    "        bordering_cells = bordering_grid_nums(row.grid_num, lat_n_gridlines, lon_n_gridlines)\n",
    "        bordering_cells = [x for x in bordering_cells if x in groupby_mean_df.index]\n",
    "        groupby_mean_df.loc[row.grid_num].fillna(groupby_mean_df.loc[bordering_cells, :].mean())\n",
    "\n",
    "    pa_train_df.loc[idx, missing_cols] = groupby_mean_df.loc[row.grid_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59172766-f28a-4f80-bba5-8e3e229b6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just fill remaining missing values with the mean\n",
    "missing_cols = pa_train_df.columns[pa_train_df.isnull().any(axis=0)]\n",
    "for column in missing_cols:\n",
    "    pa_train_df[column] = pa_train_df[column].fillna(pa_train_df[column].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e76e6363-268d-41bc-9c31-3d735faa4155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check for -Inf values\n",
    "print((pa_train_df == float(\"-Inf\")).any(axis=1).any())\n",
    "inf_cols = pa_train_df.columns[(pa_train_df == float(\"-Inf\")).any(axis=0)]\n",
    "\n",
    "for col in inf_cols:\n",
    "    inf_idxs = pa_train_df.index[pa_train_df[col] == float(\"-Inf\")]\n",
    "    pa_train_df.loc[inf_idxs, col] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03303dd1-8ee4-45f0-a05c-93755d0cdc0a",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3da6c8a-0448-472c-b9b0-ae5659470b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the two dataframes are in the same order w.r.t surveyId\n",
    "(pa_species_df.surveyId == pa_train_df.surveyId).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28611235-b959-4761-ae47-902d799377ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_species_ids = 5016\n",
    "target_speciesIds = np.random.choice(pa_metadata_train_df.speciesId.unique(), \n",
    "                                     n_species_ids, \n",
    "                                     replace=False)\n",
    "\n",
    "target_speciesIds = np.sort(pa_metadata_train_df.speciesId.unique())\n",
    "\n",
    "X = pa_train_df.drop(['surveyId', 'speciesCount', 'region', 'country', 'grid_num'], axis=1)\n",
    "y = pa_species_df.drop('surveyId', axis=1)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "005884c5-72f2-444e-833d-dda4b894c697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>year</th>\n",
       "      <th>country_Andorra</th>\n",
       "      <th>country_Austria</th>\n",
       "      <th>country_Belgium</th>\n",
       "      <th>country_Bosnia and Herzegovina</th>\n",
       "      <th>country_Bulgaria</th>\n",
       "      <th>country_Croatia</th>\n",
       "      <th>country_Czech Republic</th>\n",
       "      <th>...</th>\n",
       "      <th>swir2_10</th>\n",
       "      <th>swir2_9</th>\n",
       "      <th>swir2_8</th>\n",
       "      <th>swir2_7</th>\n",
       "      <th>swir2_6</th>\n",
       "      <th>swir2_5</th>\n",
       "      <th>swir2_4</th>\n",
       "      <th>swir2_3</th>\n",
       "      <th>swir2_2</th>\n",
       "      <th>swir2_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.099038</td>\n",
       "      <td>43.134956</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.884560</td>\n",
       "      <td>56.912140</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.256020</td>\n",
       "      <td>55.637050</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.402590</td>\n",
       "      <td>43.505630</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.517360</td>\n",
       "      <td>45.806430</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1310 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon        lat  year  country_Andorra  country_Austria  \\\n",
       "0  3.099038  43.134956  2021                0                0   \n",
       "1  9.884560  56.912140  2017                0                0   \n",
       "2  8.256020  55.637050  2019                0                0   \n",
       "3 -0.402590  43.505630  2018                0                0   \n",
       "4 -0.517360  45.806430  2017                0                0   \n",
       "\n",
       "   country_Belgium  country_Bosnia and Herzegovina  country_Bulgaria  \\\n",
       "0                0                               0                 0   \n",
       "1                0                               0                 0   \n",
       "2                0                               0                 0   \n",
       "3                0                               0                 0   \n",
       "4                0                               0                 0   \n",
       "\n",
       "   country_Croatia  country_Czech Republic  ...  swir2_10  swir2_9  swir2_8  \\\n",
       "0                0                       0  ...      17.0     13.0      9.0   \n",
       "1                0                       0  ...      31.0     34.0     20.0   \n",
       "2                0                       0  ...       8.0     15.0      9.0   \n",
       "3                0                       0  ...      15.0     38.0     35.0   \n",
       "4                0                       0  ...      33.0     28.0     38.0   \n",
       "\n",
       "   swir2_7  swir2_6  swir2_5  swir2_4  swir2_3  swir2_2  swir2_1  \n",
       "0     17.0     16.0     13.0     11.0     21.0     15.0     13.0  \n",
       "1     25.0     28.0     26.0     20.0     20.0     30.0     27.0  \n",
       "2     10.0      8.0     15.0      9.0      6.0      7.0     11.0  \n",
       "3     47.0     17.0     23.0     40.0     42.0     15.0     35.0  \n",
       "4     54.0     64.0     42.0     44.0     52.0     32.0     52.0  \n",
       "\n",
       "[5 rows x 1310 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c0de752-1fef-4fa0-bfd3-0f49aca5aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('X_train.csv', index=False)\n",
    "y.to_csv('y_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e31758-29a4-4b75-aa10-bfe9999e1c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
